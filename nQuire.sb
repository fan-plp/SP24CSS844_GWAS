#!/bin/bash

##### SLURM Resource Requests #####

#SBATCH --time=6-23:10				#How long the job will run (days-hours:minutes)
#SBATCH --nodes=1				#How many compute nodes the job needs
#SBATCH --ntasks=1				#How many concurrent tasks the job needs 
#SBATCH --cpus-per-task=8			#How many CPUs each task needs 
#SBATCH --mem-per-cpu=20G			#How much memory each CPU needs

##### SLURM Administrative Settings #####

#SBATCH --job-name=nquire			#Name the job for convenience 
#SBATCH --mail-type=ALL				#Tell SLURM to email you when job starts, stops, error
#SBATCH --mail-user=fanqiuro@msu.edu 				#Provide SLURM your email address


##### bash  Commands to Run #####

## this is for determining the ploidy level of each genome type. the script uses GMM 
#aims to model the read frequency histogram as a mixture of up to three Gaussian distributions between 0 and 1, 
#that are scaled relatively to each other by some mixture proportion. 
#the expected distributions of read frequencies at biallelic sites for each of our ploidy levels of interest are one Gaussian with mean 0.5 for diploid, 
#two Gaussians with means 0.33 and 0.67 for triploid, and three Gaussians with means 0.25, 0.5 and 0.75 for tetraploid.
#see documentation at https://github.com/clwgg/nQuire

cd /mnt/scratch/fanqiuro/Potato_GWAS
mkdir nQuirebin 
cd nQuirebin

in_dir  = "/mnt/scratch/fanqiuro/Potato_GWAS/bwamapped/" # we are using bwa mapped reads for assesing the ploidy level 

for x in 'cat listlane1'
do 
    nQuire create -b $in_dir"$x".bam -o "$x" 
    nQuire denoise "$x".bin -o "$x"_denoised
    nQuire lrdmodel -t 8 "$x"_denoised.bin
done

#repeat this process for listlane2 and 3

# now the file will generate output text containig file 8 tab-separated columns, we will import these data to R and check the distribution using histgram

